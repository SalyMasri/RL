# ğŸ† Reinforcement Learning: Q-Learning Agent

## ğŸ“Œ Project Overview
This project implements a **Q-Learning agent** for a grid-based environment. The goal is to optimize the agent's decision-making process by balancing **exploration** and **exploitation** using reinforcement learning techniques. The agent learns an optimal policy to maximize rewards while navigating through various states.

## ğŸ› ï¸ Technologies & Tools Used
- **Programming Language**: Python
- **Libraries**: NumPy
- **Algorithm**: Q-Learning
- **Optimization Methods**: Epsilon-Greedy Strategy, Bellman Update

## ğŸ” Key Features
- **State Representation**: The environment is represented as a **discretized grid** where actions correspond to movement.
- **Action Selection**: Implemented **Epsilon-Greedy Policy** to control exploration vs. exploitation.
- **Q-Table Initialization**: Uses **NaN masking** to prevent invalid moves.
- **Bellman Update**: Computes **Q-values** iteratively to converge towards the optimal policy.
- **Early Stopping Criterion**: Convergence is determined using **mean Q-value change**.

## ğŸ“Š Results
âœ”ï¸ The agent successfully **learns an optimal policy** to achieve the highest possible reward.  
âœ”ï¸ Performance is influenced by **learning rate (Î±), discount factor (Î³), and epsilon decay**.  
âœ”ï¸ Different hyperparameters significantly affect the training efficiency and final policy stability.

## ğŸ”„ Future Improvements
- Implement **Deep Q-Networks (DQN)** for function approximation.
- Experiment with **different reward functions** to fine-tune performance.
- Improve **convergence efficiency** by optimizing the learning rate schedule.

---
